import torch
import numpy as np
import os
import json
import yaml
from typing import List, Dict, Optional
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import hf_hub_download, snapshot_download

# Try importing vLLM
try:
    from vllm import LLM, SamplingParams
    USE_VLLM = True
except ImportError:
    print("[WARNING] vLLM not found. Falling back to Transformers.")
    USE_VLLM = False
    from transformers import AutoModelForCausalLM

from .embedding import KmBertEmbedder
from .module_augment import build_prompt
from .data_loader import load_documents_from_dirs

class VetRAGPipeline:
    def __init__(self, config_path="config.yaml", doc_dir=None):
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)
            
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"[INFO] Initializing VetRAG Pipeline on {self.device}...")
        
        # Rationale Generation ì„¤ì • (RAG^2-style Query Enhancement)
        rationale_config = self.config.get("rationale_gen", {})
        self.use_rationale = rationale_config.get("enabled", True)
        
        # 1. Load Retrieval Model (Bi-Encoder)
        retrieval_model_name = self.config["retrieval"]["model_name"]
        print(f"[1/3] Loading Retrieval Model: {retrieval_model_name}")
        self.embedder = KmBertEmbedder(model_name=retrieval_model_name, device=self.device)
        
        # 2. Load Reranking Model (Cross-Encoder)
        # Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ë¡œì»¬ ê²½ë¡œì—ì„œ ë¡œë“œ
        rerank_config = self.config.get("training", {})
        hf_repo_id = rerank_config.get("huggingface_repo_id", None)
        local_model_path = f"{rerank_config.get('output_dir', './results/bert_top25percent')}/final_model"
        
        print(f"[2/3] Loading Reranking Model...")
        
        # Hugging Face ë¦¬í¬ì§€í† ë¦¬ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if hf_repo_id:
            print(f"   ğŸ“¦ Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {hf_repo_id}")
            try:
                # Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (í† í° ë¶ˆí•„ìš”, Public ë¦¬í¬ì§€í† ë¦¬)
                # snapshot_downloadì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ëª¨ë¸ í´ë” ë‹¤ìš´ë¡œë“œ
                downloaded_path = snapshot_download(
                    repo_id=hf_repo_id,
                    repo_type="model",
                    local_files_only=False  # ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
                )
                print(f"   âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_path}")
                self.rerank_tokenizer = AutoTokenizer.from_pretrained(downloaded_path)
                self.rerank_model = AutoModelForSequenceClassification.from_pretrained(downloaded_path).to(self.device)
                self.rerank_model.eval()
                print(f"   âœ… Reranking ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸  Hugging Faceì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"   ğŸ”„ ë¡œì»¬ ê²½ë¡œì—ì„œ ì‹œë„: {local_model_path}")
                # ë¡œì»¬ ê²½ë¡œë¡œ fallback
                if os.path.exists(local_model_path):
                    try:
                        self.rerank_tokenizer = AutoTokenizer.from_pretrained(local_model_path)
                        self.rerank_model = AutoModelForSequenceClassification.from_pretrained(local_model_path).to(self.device)
                        self.rerank_model.eval()
                        print(f"   âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    except Exception as e2:
                        print(f"   âŒ ë¡œì»¬ ëª¨ë¸ ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                        print(f"   ğŸ”„ Base ëª¨ë¸ ì‚¬ìš©: {retrieval_model_name}")
                        self.rerank_tokenizer = AutoTokenizer.from_pretrained(retrieval_model_name)
                        self.rerank_model = AutoModelForSequenceClassification.from_pretrained(retrieval_model_name, num_labels=2).to(self.device)
                else:
                    print(f"   âŒ ë¡œì»¬ ê²½ë¡œë„ ì¡´ì¬í•˜ì§€ ì•ŠìŒ. Base ëª¨ë¸ ì‚¬ìš©: {retrieval_model_name}")
                    self.rerank_tokenizer = AutoTokenizer.from_pretrained(retrieval_model_name)
                    self.rerank_model = AutoModelForSequenceClassification.from_pretrained(retrieval_model_name, num_labels=2).to(self.device)
        else:
            # Hugging Face ë¦¬í¬ì§€í† ë¦¬ê°€ ì—†ìœ¼ë©´ ë¡œì»¬ ê²½ë¡œì—ì„œ ë¡œë“œ
            print(f"   ğŸ“ ë¡œì»¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë“œ: {local_model_path}")
            try:
                if os.path.exists(local_model_path):
                    self.rerank_tokenizer = AutoTokenizer.from_pretrained(local_model_path)
                    self.rerank_model = AutoModelForSequenceClassification.from_pretrained(local_model_path).to(self.device)
                    self.rerank_model.eval()
                    print(f"   âœ… ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    raise FileNotFoundError(f"ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {local_model_path}")
            except Exception as e:
                print(f"   âš ï¸  ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"   ğŸ”„ Base ëª¨ë¸ ì‚¬ìš©: {retrieval_model_name}")
                self.rerank_tokenizer = AutoTokenizer.from_pretrained(retrieval_model_name)
                self.rerank_model = AutoModelForSequenceClassification.from_pretrained(retrieval_model_name, num_labels=2).to(self.device)
        
        # 3-1. Load Answer Generation Model (LLM)
        llm_config = self.config.get("llm", {})
        llm_model_name = llm_config.get("model_name", "gpt2")
        print(f"[3/4] Loading Answer Generation Model: {llm_model_name}")
        
        if USE_VLLM:
            try:
                self.llm = LLM(
                    model=llm_model_name,
                    quantization=llm_config.get("quantization"),
                    gpu_memory_utilization=llm_config.get("gpu_memory_utilization", 0.9),
                    max_model_len=llm_config.get("max_model_len", 4096),
                    dtype=llm_config.get("dtype", "float16"),
                    trust_remote_code=True
                )
                self.sampling_params = SamplingParams(
                    temperature=0.7,
                    top_p=0.95,
                    max_tokens=512,
                    repetition_penalty=1.2
                )
                print("[SUCCESS] Answer Generation Model (vLLM) loaded successfully.")
            except Exception as e:
                print(f"[ERROR] Failed to load vLLM: {e}. Falling back to Transformers.")
                self.use_vllm_fallback(llm_model_name, is_answer_gen=True)
        else:
            self.use_vllm_fallback(llm_model_name, is_answer_gen=True)
        
        # 3-2. Rationale Generation Model ì„¤ì • (ì§€ì—° ë¡œë”©: í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)
        rationale_model_name = rationale_config.get("model_name", None)
        if rationale_model_name and self.use_rationale:
            print(f"[4/4] Rationale Generation Model: {rationale_model_name} (will be loaded on-demand)")
            self.rationale_model_name = rationale_model_name
            self.rationale_llm_tokenizer = None
            self.rationale_llm_model = None
        else:
            # Rationale ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„í™œì„±í™”ëœ ê²½ìš°, Answer Generation ëª¨ë¸ ì¬ì‚¬ìš©
            print(f"[4/4] Rationale Generation Model not specified. Using Answer Generation Model.")
            self.rationale_model_name = llm_model_name
            self.rationale_llm_tokenizer = self.llm_tokenizer
            self.rationale_llm_model = self.llm_model
        
        # Rationale Generation í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í‚¤ì›Œë“œ í™•ì¥ ì¤‘ì‹¬)
        self.rationale_prompt_template = rationale_config.get(
            "prompt_template",
            """[ì—­í• ]
ë‹¹ì‹ ì€ ìˆ˜ì˜í•™ RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•œ 'ê²€ìƒ‰ì–´ í™•ì¥ ì „ë¬¸ê°€'ì…ë‹ˆë‹¤.

[ì…ë ¥ ì§ˆë¬¸]
{query}

[ëª©í‘œ]
- ìœ„ ì§ˆë¬¸ì— ëŒ€í•´, ìˆ˜ì˜í•™ ë…¼ë¬¸/ì „ë¬¸ ì„œì ì— ë“±ì¥í•  ë²•í•œ í•µì‹¬ ì˜í•™ í‚¤ì›Œë“œë§Œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì¼ìƒ í‘œí˜„ì„, ë¬¸ì„œì— ë“±ì¥í•  ìˆ˜ì˜í•™ ì „ë¬¸ ìš©ì–´/ê´€ë ¨ ì§ˆí™˜ëª…/ì¦ìƒëª…/ê²€ì‚¬ëª…ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹ ê·œì¹™]
- í•œêµ­ì–´ í‚¤ì›Œë“œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- í•œê¸€, ìˆ«ì, ê³µë°±ë§Œ ì‚¬ìš©í•˜ê³  ë”°ì˜´í‘œ("), í™”ì‚´í‘œ(->, â†’), ê´„í˜¸(), ì±… ì œëª©, ì—°ë„(2018, 2020 ë“±)ëŠ” ì ˆëŒ€ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ë¬¸ì¥ìœ¼ë¡œ ì“°ì§€ ë§ê³  í‚¤ì›Œë“œë§Œ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ í•œ ì¤„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
- 3~8ê°œì˜ ì§§ì€ í‚¤ì›Œë“œë§Œ ìƒì„±í•©ë‹ˆë‹¤.
- ì˜ˆì‹œì²˜ëŸ¼, ì¦ìƒ + ê´€ë ¨ ì§ˆí™˜/ì¥ê¸°/ì¦ìƒêµ° ì¡°í•©ì„ ê°„ë‹¨íˆ ì„ì–´ ì¤ë‹ˆë‹¤.

[ì˜ˆì‹œ]
- ì§ˆë¬¸: ê°•ì•„ì§€ê°€ ë…¸ë€ í† ë¥¼ í•´ìš”.
- ê°€ëŠ¥í•œ ì¶œë ¥: êµ¬í† , ê³µë³µí† , ë‹´ì¦™ ì—­ë¥˜, ìœ„ì¥ê´€ ì§ˆí™˜, ì†Œí™”ê¸° ì§ˆí™˜
- ì˜ëª»ëœ ì¶œë ¥: ê°•ì•„ì§€ê°€ ë…¸ë€ í† ë¥¼ í•©ë‹ˆë‹¤, ë™ë¬¼ë³‘ì›ì˜ì‚¬ë“¤ì´ì¶”ì²œí•˜ëŠ”ë°˜ë ¤ê²¬ê±´ê°•ë°±ì„œ 2018

[ìµœì¢… ì¶œë ¥]
ìœ„ ê·œì¹™ì„ ë§Œì¡±í•˜ëŠ” í‚¤ì›Œë“œ ëª©ë¡ë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„í•´ ì¶œë ¥í•˜ì„¸ìš”.
í‚¤ì›Œë“œ:"""
        )
        
        # 4. Load Documents & Embeddings
        self.documents = []
        self.doc_embeddings = None
        
        kb_dirs = None
        if doc_dir is not None:
            kb_dirs = doc_dir
        else:
            kb_dirs = self.config.get("knowledge_base", {}).get("directories")
        if kb_dirs:
            print(f"[INFO] Starting to load knowledge base from {len(kb_dirs) if isinstance(kb_dirs, list) else 1} directory(ies)...")
            self.load_knowledge_base(kb_dirs)
            print(f"[SUCCESS] Knowledge base loaded: {len(self.documents)} documents.")
        else:
            print("[WARNING] No knowledge base directories provided. Retrieval will not work until documents are loaded.")
            
    def use_vllm_fallback(self, model_name, is_answer_gen=True):
        """
        Transformersë¡œ ëª¨ë¸ ë¡œë“œ (vLLM ì‹¤íŒ¨ ì‹œ ë˜ëŠ” ì˜ë£Œ íŠ¹í™” ëª¨ë¸ìš©)
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            is_answer_gen: Trueë©´ Answer Generationìš©, Falseë©´ Rationale Generationìš©
        """
        from transformers import AutoModelForCausalLM, BitsAndBytesConfig
        
        print(f"[INFO] Loading {model_name} with BitsAndBytes 4-bit quantization...")
        
        # Answer ëª¨ë¸ì´ ì´ë¯¸ í•´ì œë˜ì—ˆìœ¼ë¯€ë¡œ GPUì— ì¶©ë¶„í•œ ê³µê°„ì´ ìˆìŒ
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        device_map = "auto"  # GPUì— ë¡œë“œ
        
        print(f"[INFO] Loading tokenizer for {model_name}...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print(f"[INFO] Loading model (this may take a while)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            trust_remote_code=True, 
            device_map=device_map,
            quantization_config=quantization_config,
            low_cpu_mem_usage=True
        )
        model.eval()
        print(f"[INFO] Model loaded and set to eval mode.")
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        if is_answer_gen:
            self.llm_tokenizer = tokenizer
            self.llm_model = model
            print(f"[SUCCESS] Answer Generation model ready.")
        else:
            self.rationale_llm_tokenizer = tokenizer
            self.rationale_llm_model = model
            print(f"[SUCCESS] Rationale Generation model ready.")
            
    def load_knowledge_base(self, doc_dirs):
        if isinstance(doc_dirs, str):
            doc_dirs = [doc_dirs]
        print(f"[INFO] Loading documents from {len(doc_dirs)} directory(ies)...")
        self.documents = load_documents_from_dirs(doc_dirs)
        
        # Load or compute embeddings
        cache_path = "cache/doc_embeddings.npy"
        if os.path.exists(cache_path):
            print("[INFO] Loading cached document embeddings...")
            self.doc_embeddings = np.load(cache_path)
            
            # Verify shape match
            if len(self.documents) != self.doc_embeddings.shape[0]:
                print(f"[WARNING] Document count ({len(self.documents)}) does not match embedding count ({self.doc_embeddings.shape[0]}). Recomputing...")
                self.doc_embeddings = self.embedder.encode(self.documents)
                np.save(cache_path, self.doc_embeddings)
        else:
            print("[INFO] Computing document embeddings (this may take a while)...")
            self.doc_embeddings = self.embedder.encode(self.documents)
            os.makedirs("cache", exist_ok=True)
            np.save(cache_path, self.doc_embeddings)
            
    def _load_rationale_model(self):
        """
        Rationale Generation ëª¨ë¸ì„ ì§€ì—° ë¡œë”©í•©ë‹ˆë‹¤.
        í•„ìš”í•  ë•Œë§Œ ë¡œë“œí•˜ê³  ì‚¬ìš© í›„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤.
        Answer ëª¨ë¸ì„ ë¨¼ì € í•´ì œí•œ í›„ Rationale ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        if self.rationale_llm_model is not None:
            return  # ì´ë¯¸ ë¡œë“œë¨
        
        if not self.rationale_model_name or self.rationale_model_name == self.config.get("llm", {}).get("model_name"):
            # Answer Generation ëª¨ë¸ ì¬ì‚¬ìš©
            self.rationale_llm_tokenizer = self.llm_tokenizer
            self.rationale_llm_model = self.llm_model
            return
        
        # Answer ëª¨ë¸ì„ ë¨¼ì € ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ (GPU ë©”ëª¨ë¦¬ í™•ë³´)
        print(f"[INFO] Unloading Answer Generation model to free GPU memory...")
        if hasattr(self, 'llm_model') and self.llm_model is not None:
            del self.llm_model
            del self.llm_tokenizer
            self.llm_model = None
            self.llm_tokenizer = None
            torch.cuda.empty_cache()
            print(f"[SUCCESS] Answer Generation model unloaded.")
        
        print(f"[INFO] Loading Rationale Generation Model: {self.rationale_model_name} (on-demand)...")
        self.use_vllm_fallback(self.rationale_model_name, is_answer_gen=False)
        print(f"[SUCCESS] Rationale Generation model loaded.")
    
    def _unload_rationale_model(self):
        """
        Rationale Generation ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤.
        Answer ëª¨ë¸ ì¬ë¡œë”©ì€ generate ë©”ì„œë“œì—ì„œ í•„ìš”í•  ë•Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if self.rationale_llm_model is not None and self.rationale_llm_model != self.llm_model:
            print(f"[INFO] Unloading Rationale Generation model from GPU memory...")
            del self.rationale_llm_model
            del self.rationale_llm_tokenizer
            self.rationale_llm_model = None
            self.rationale_llm_tokenizer = None
            # ê°•ë ¥í•œ ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            print(f"[SUCCESS] Rationale Generation model unloaded.")
    
    def generate_rationale(self, query: str) -> str:
        """
        RAG^2-style Rationale Generation (Query Expansion)
        ì˜ë£Œ íŠ¹í™” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ í™•ì¥: ì›ë³¸ ì§ˆë¬¸ + ì˜í•™ ì „ë¬¸ ìš©ì–´ í‚¤ì›Œë“œ
        ëª¨ë¸ì€ í•„ìš”í•  ë•Œë§Œ ë¡œë“œí•˜ê³  ì‚¬ìš© í›„ í•´ì œí•©ë‹ˆë‹¤.
        """
        prompt = self.rationale_prompt_template.format(query=query)
        
        # Rationale ëª¨ë¸ ì§€ì—° ë¡œë”©
        self._load_rationale_model()
        rationale_tokenizer = self.rationale_llm_tokenizer
        rationale_model = self.rationale_llm_model
        
        try:
            inputs = rationale_tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            ).to(self.device)
            
            input_length = inputs.input_ids.shape[1]
            
            with torch.no_grad():
                outputs = rationale_model.generate(
                    **inputs,
                    max_new_tokens=self.config.get("rationale_gen", {}).get("max_tokens", 128),
                    do_sample=True,
                    temperature=self.config.get("rationale_gen", {}).get("temperature", 0.1),
                    top_p=self.config.get("rationale_gen", {}).get("top_p", 0.9),
                    repetition_penalty=1.2,
                    pad_token_id=rationale_tokenizer.eos_token_id,
                    eos_token_id=rationale_tokenizer.eos_token_id
                )
            
            # ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
            generated_ids = outputs[0][input_length:]
            rationale_text = rationale_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            
            # Rationale ëª¨ë¸ í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if rationale_model != self.llm_model:
                self._unload_rationale_model()
            
            # í‚¤ì›Œë“œ íŒŒì‹±: ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = []
            if rationale_text:
                # "í‚¤ì›Œë“œ:" ê°™ì€ ì ‘ë‘ì‚¬ ì œê±°
                rationale_text = rationale_text.split(":", 1)[-1].strip()
                # ì¤„ë°”ê¿ˆ/ë¹„í‘œì¤€ ê³µë°± ì œê±°
                rationale_text = rationale_text.replace("\n", " ").replace("\t", " ")
                # ì½¤ë§ˆë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°
                raw_keywords = [kw.strip() for kw in rationale_text.split(",") if kw.strip()]
                cleaned_keywords = []
                for kw in raw_keywords:
                    # ë”°ì˜´í‘œ, í™”ì‚´í‘œ, ê´„í˜¸, íŠ¹ìˆ˜ë¬¸ì ì œê±°
                    for ch in ['"', "'", "â€œ", "â€", "â€˜", "â€™", "â†’", "->", "(", ")", "[", "]"]:
                        kw = kw.replace(ch, " ")
                    kw = kw.strip()
                    # ë„ˆë¬´ ê¸´ í•­ëª© ì œê±° (ë¬¸ì¥ì´ ì•„ë‹Œ í‚¤ì›Œë“œë§Œ)
                    if not kw or len(kw) > 25:
                        continue
                    cleaned_keywords.append(kw)
                keywords = cleaned_keywords
            
            # ë¹ˆ ì‘ë‹µ ì²´í¬ ë˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ë§Œ ì‚¬ìš©
            if not keywords:
                print(f"[WARNING] [Rationale] No keywords extracted. Using original query only.")
                return query
            
            # Query Expansion: ì›ë³¸ ì§ˆë¬¸ + í‚¤ì›Œë“œ ê²°í•©
            expanded_query = f"{query} [SEP] {', '.join(keywords)}"
            print(f"[INFO] [Rationale] Extracted keywords: {keywords}")

            # ë‚˜ì¤‘ì— ë¶„ì„/ì €ì¥ì„ ìœ„í•´ ë§ˆì§€ë§‰ Rationale ì •ë³´ ë³´ê´€
            self.last_rationale = {
                "original_query": query,
                "expanded_query": expanded_query,
                "keywords": keywords,
            }
            return expanded_query
            
        except Exception as e:
            print(f"[WARNING] [Rationale] Error during generation: {e}. Using original query.")
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ëª¨ë¸ í•´ì œ
            if rationale_model != self.llm_model:
                self._unload_rationale_model()
            # ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œ ì •ë³´ëŠ” ê¸°ë¡
            self.last_rationale = {
                "original_query": query,
                "expanded_query": query,
                "keywords": [],
            }
            return query
    
    def retrieve(self, query: str, top_n: int = 100, use_rationale: bool = None) -> List[Dict]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_n: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜
            use_rationale: Rationale ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ config ì„¤ì • ë”°ë¦„)
        """
        # Step 0: Rationale Generation (RAG^2-style)
        if use_rationale is None:
            use_rationale = self.use_rationale
        
        if use_rationale:
            print(f"[INFO] [Rationale] Original Query: {query}")
            expanded_query = self.generate_rationale(query)
            print(f"[INFO] [Rationale] Expanded Query: {expanded_query[:150]}...")
            search_query = expanded_query
        else:
            search_query = query
        
        # Step 1: Retrieval (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©)
        q_emb = self.embedder.encode(search_query)
        
        # ì„ë² ë”© ì •ê·œí™” (L2 norm)
        q_emb_norm = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-8)
        doc_embs_norm = self.doc_embeddings / (np.linalg.norm(self.doc_embeddings, axis=1, keepdims=True) + 1e-8)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì •ê·œí™”ëœ ë²¡í„°ì˜ ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        sims = np.dot(doc_embs_norm, q_emb_norm.T).flatten()
        top_indices = np.argsort(sims)[::-1][:top_n]
        
        candidates = []
        for idx in top_indices:
            candidates.append({
                "chunk_id": int(idx),
                "chunk_text": self.documents[idx],
                "score": float(sims[idx])  # ì´ì œ -1~1 ë²”ìœ„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            })
        return candidates
        
    def rerank(self, query: str, candidates: List[Dict], top_k: int = 5, min_score: float = 0.5) -> List[Dict]:
        """
        Rerank candidates using cross-encoder model.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            candidates: ê²€ìƒ‰ëœ í›„ë³´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜
            min_score: ìµœì†Œ reranking ì ìˆ˜ (ì´ ì ìˆ˜ ë¯¸ë§Œì€ ì œì™¸)
        """
        if not candidates:
            return []
        
        # Step 2: Reranking
        pairs = [[query, doc["chunk_text"]] for doc in candidates]
        
        inputs = self.rerank_tokenizer(
            pairs, padding=True, truncation=True, return_tensors="pt", max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            logits = self.rerank_model(**inputs).logits
            probs = torch.softmax(logits, dim=1)
            scores = probs[:, 1].cpu().numpy()
            
        for i, doc in enumerate(candidates):
            doc["rerank_score"] = float(scores[i])
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìµœì†Œ ì ìˆ˜ ì´ìƒë§Œ í•„í„°ë§
        reranked = sorted(candidates, key=lambda x: x["rerank_score"], reverse=True)
        reranked = [doc for doc in reranked if doc["rerank_score"] >= min_score]
        
        # ìƒìœ„ kê°œë§Œ ë°˜í™˜
        return reranked[:top_k]
        
    def _ensure_answer_model_loaded(self):
        """
        Answer Generation ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        if self.llm_model is None or self.llm_tokenizer is None:
            print(f"[INFO] Answer Generation model not loaded. Reloading...")
            # ì¶”ê°€ ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            torch.cuda.empty_cache()
            
            llm_config = self.config.get("llm", {})
            llm_model_name = llm_config.get("model_name", "gpt2")
            self.use_vllm_fallback(llm_model_name, is_answer_gen=True)
            print(f"[SUCCESS] Answer Generation model reloaded.")
    
    def generate(self, query: str, context_docs: List[Dict]) -> str:
        # Step 3: Generation
        # Answer ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        self._ensure_answer_model_loaded()
        
        prompt = build_prompt(query, context_docs)
        
        if USE_VLLM:
            outputs = self.llm.generate([prompt], self.sampling_params)
            response = outputs[0].outputs[0].text.strip()
            return response
        else:
            # Llama2/Qwen ëª¨ë¸ì˜ ê²½ìš° ì±„íŒ… í…œí”Œë¦¿ ì‚¬ìš© ì‹œë„
            try:
                # Qwen2.5 ì±„íŒ… í…œí”Œë¦¿ ì‚¬ìš© (ê°•í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
                if hasattr(self.llm_tokenizer, 'apply_chat_template'):
                    system_content = (
                        "ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì˜í•™ ì •ë³´ë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ìˆ˜ì˜ì‚¬ AIì…ë‹ˆë‹¤.\n"
                        "ë°˜ë“œì‹œ ì œê³µëœ ê·¼ê±°ë§Œì„ í™œìš©í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
                        "\n"
                        "ã€ì¤‘ìš” ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”ã€‘\n"
                        "1. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì¤‘êµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
                        "2. ë¬¸ë§¥ì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ê±°ë‚˜ ê·¼ê±°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë°˜ë“œì‹œ 'ì •ë³´ ë¶€ì¡±'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
                        "3. ë¬¸ë§¥ì— ì—†ëŠ” ì˜í•™ ì§€ì‹ì´ë‚˜ ì•½ë¬¼ ì •ë³´ë¥¼ ìƒˆë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.\n"
                        "4. ê·¼ê±°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ 'ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.' ë˜ëŠ” 'ì •ë³´ ë¶€ì¡±'ì´ë¼ê³  ëª…í™•íˆ ë°íˆì„¸ìš”.\n"
                        "\n"
                        "ã€ì–¸ì–´ ê·œì¹™ã€‘\n"
                        "- ë‹µë³€ì€ 100% í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
                        "- ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì„ìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n"
                        "- ì¤‘êµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“± ì–´ë–¤ ë‹¤ë¥¸ ì–¸ì–´ë„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
                    )
                    messages = [
                        {"role": "system", "content": system_content},
                        {"role": "user", "content": prompt}
                    ]
                    formatted_prompt = self.llm_tokenizer.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                else:
                    # ì±„íŒ… í…œí”Œë¦¿ì´ ì—†ëŠ” ê²½ìš° ê°•í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    system_msg = (
                        "ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì˜í•™ ì •ë³´ë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ìˆ˜ì˜ì‚¬ AIì…ë‹ˆë‹¤.\n"
                        "ë°˜ë“œì‹œ ì œê³µëœ ê·¼ê±°ë§Œì„ í™œìš©í•´ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
                        "\n"
                        "ã€ì¤‘ìš” ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”ã€‘\n"
                        "1. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì¤‘êµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
                        "2. ë¬¸ë§¥ì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ê±°ë‚˜ ê·¼ê±°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë°˜ë“œì‹œ 'ì •ë³´ ë¶€ì¡±'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
                        "3. ë¬¸ë§¥ì— ì—†ëŠ” ì˜í•™ ì§€ì‹ì´ë‚˜ ì•½ë¬¼ ì •ë³´ë¥¼ ìƒˆë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.\n"
                        "4. ê·¼ê±°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ 'ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.' ë˜ëŠ” 'ì •ë³´ ë¶€ì¡±'ì´ë¼ê³  ëª…í™•íˆ ë°íˆì„¸ìš”.\n"
                        "\n"
                        "ã€ì–¸ì–´ ê·œì¹™ã€‘\n"
                        "- ë‹µë³€ì€ 100% í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
                        "- ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì„ìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n"
                        "- ì¤‘êµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“± ì–´ë–¤ ë‹¤ë¥¸ ì–¸ì–´ë„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
                    )
                    formatted_prompt = f"<s>[INST] <<SYS>>\n{system_msg}\n<</SYS>>\n\n{prompt} [/INST]"
            except Exception as e:
                print(f"[WARNING] Failed to apply chat template: {e}. Using raw prompt.")
                formatted_prompt = prompt
            
            inputs = self.llm_tokenizer(
                formatted_prompt, 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            input_length = inputs.input_ids.shape[1]
            
            with torch.no_grad():
                outputs = self.llm_model.generate(
                    **inputs, 
                    max_new_tokens=512,
                    do_sample=True,
                    temperature=0.7,
                    repetition_penalty=1.2,
                    pad_token_id=self.llm_tokenizer.eos_token_id,
                    eos_token_id=self.llm_tokenizer.eos_token_id
                )
            
            # ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
            generated_ids = outputs[0][input_length:]
            response = self.llm_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            
            # ì¶”ê°€ ì •ë¦¬: í”„ë¡¬í”„íŠ¸ê°€ ì—¬ì „íˆ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œê±°
            if "[ê·¼ê±°]" in response or "[ì§ˆë¬¸]" in response:
                # "[ë‹µë³€]" ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if "[ë‹µë³€]" in response:
                    response = response.split("[ë‹µë³€]", 1)[-1].strip()
                # í”„ë¡¬í”„íŠ¸ì˜ ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œê±°
                if "ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼" in response:
                    lines = response.split("\n")
                    filtered_lines = []
                    skip_until_answer = False
                    for line in lines:
                        if "ë‹µë³€" in line or "### ë‹µë³€" in line:
                            skip_until_answer = True
                        if skip_until_answer and "ë‹¹ì‹ ì€" not in line and "ë°˜ë ¤ë™ë¬¼" not in line:
                            filtered_lines.append(line)
                    response = "\n".join(filtered_lines).strip()
            
            return response

    def run(self, query: str, use_rationale: bool = None) -> str:
        """
        ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            use_rationale: Rationale ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ config ì„¤ì • ë”°ë¦„)
        """
        print(f"\n[INFO] Original Query: {query}")
        
        # 0. Rationale Generation (RAG^2-style)
        if use_rationale is None:
            use_rationale = self.use_rationale
        
        if use_rationale:
            print(f"\n{'='*60}")
            print("STEP 0: Rationale Generation (RAG^2)")
            print(f"{'='*60}")
            refined_query = self.generate_rationale(query)
            print(f"[SUCCESS] Rationale generated. Using refined query for retrieval.")
        else:
            refined_query = query
        
        # 1. Retrieve
        print(f"\n{'='*60}")
        print("STEP 1: Retrieval")
        print(f"{'='*60}")
        candidates = self.retrieve(query, top_n=50, use_rationale=use_rationale)
        print(f"[SUCCESS] Retrieved {len(candidates)} candidates.")
        
        # 2. Rerank
        print(f"\n{'='*60}")
        print("STEP 2: Reranking")
        print(f"{'='*60}")
        top_docs = self.rerank(query, candidates, top_k=3)
        print(f"[SUCCESS] Reranked top {len(top_docs)} documents.")
        for i, doc in enumerate(top_docs):
            print(f"   [{i+1}] Score: {doc['rerank_score']:.4f} | {doc['chunk_text'][:50]}...")
            
        # 3. Generate
        print(f"\n{'='*60}")
        print("STEP 3: Answer Generation")
        print(f"{'='*60}")
        answer = self.generate(query, top_docs)
        
        print("\n" + "="*60)
        print("[INFO] Final Answer:")
        print("="*60)
        print(answer)
        print("="*60)
        return answer
